# Supabase å‘é‡æ•°æ®åº“ä¼˜åŒ–æ–¹æ¡ˆ

## ğŸ“‹ ä¼˜åŒ–ç›®æ ‡

è§£å†³ LLM token è¶…å‡ºé™åˆ¶é—®é¢˜ï¼Œé€šè¿‡å‘é‡æ•°æ®åº“å®ç°ï¼š

- æ™ºèƒ½ä»£ç ç‰‡æ®µæ£€ç´¢
- é¡¹ç›®ä¸Šä¸‹æ–‡å‹ç¼©
- å†å²å¯¹è¯ä¼˜åŒ–
- ç»„ä»¶çŸ¥è¯†åº“ç®¡ç†

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### 1. å‘é‡ç»´åº¦é…ç½®

**é‡è¦è¯´æ˜ï¼š** æ‰€æœ‰å‘é‡è¡¨éƒ½ä½¿ç”¨ **1536 ç»´** å‘é‡ï¼Œè¿™ä¸ Azure OpenAI çš„ `text-embedding-3-large` æ¨¡å‹å…¼å®¹ã€‚

- æ•°æ®åº“ schema ä¸­å®šä¹‰çš„å‘é‡ç»´åº¦ï¼š`vector(1536)`
- Embedding æœåŠ¡ä¸­æŒ‡å®šç”Ÿæˆ 1536 ç»´å‘é‡ï¼š`dimensions: 1536`
- è¿™ç¡®ä¿äº†æ¨¡å‹è¾“å‡ºä¸æ•°æ®åº“å­˜å‚¨çš„ç»´åº¦å®Œå…¨åŒ¹é…

### 2. å‘é‡æ•°æ®åº“ç»“æ„

```sql
-- å¯ç”¨å‘é‡æ‰©å±•
CREATE EXTENSION IF NOT EXISTS vector;

-- ä»£ç ç‰‡æ®µå‘é‡è¡¨
CREATE TABLE code_embeddings (
  id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
  project_id TEXT NOT NULL,
  file_path TEXT NOT NULL,
  content_type TEXT NOT NULL, -- 'component', 'function', 'type', 'style'
  code_snippet TEXT NOT NULL,
  description TEXT,
  tags TEXT[],
  embedding vector(1536), -- OpenAI ada-002 ç»´åº¦
  metadata JSONB,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- é¡¹ç›®ä¸Šä¸‹æ–‡å‘é‡è¡¨
CREATE TABLE project_context_embeddings (
  id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
  project_id TEXT NOT NULL,
  context_type TEXT NOT NULL, -- 'structure', 'dependencies', 'config', 'api'
  content TEXT NOT NULL,
  summary TEXT,
  embedding vector(1536),
  importance_score FLOAT DEFAULT 0.5,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- å¯¹è¯å†å²å‘é‡è¡¨
CREATE TABLE conversation_embeddings (
  id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
  conversation_id TEXT NOT NULL,
  project_id TEXT,
  user_intent TEXT NOT NULL,
  ai_response_summary TEXT,
  embedding vector(1536),
  tokens_saved INTEGER DEFAULT 0,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ç»„ä»¶åº“çŸ¥è¯†å‘é‡è¡¨
CREATE TABLE component_knowledge_embeddings (
  id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
  component_name TEXT NOT NULL,
  component_path TEXT NOT NULL,
  props_info TEXT,
  usage_examples TEXT,
  related_components TEXT[],
  embedding vector(1536),
  usage_frequency INTEGER DEFAULT 0,
  last_used_at TIMESTAMP WITH TIME ZONE,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX code_embeddings_embedding_idx ON code_embeddings
USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

CREATE INDEX project_context_embedding_idx ON project_context_embeddings
USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

CREATE INDEX conversation_embedding_idx ON conversation_embeddings
USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

CREATE INDEX component_knowledge_embedding_idx ON component_knowledge_embeddings
USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
```

### 2. å‘é‡åŒ–æœåŠ¡æ¶æ„

```typescript
// src/lib/vector/embedding-service.ts
import { createClient } from "@supabase/supabase-js";
import OpenAI from "openai";

export class EmbeddingService {
  private supabase;
  private openai;

  constructor() {
    this.supabase = createClient(
      process.env.SUPABASE_URL!,
      process.env.SUPABASE_ANON_KEY!
    );
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY!,
    });
  }

  /**
   * ç”Ÿæˆæ–‡æœ¬å‘é‡
   */
  async generateEmbedding(text: string): Promise<number[]> {
    const response = await this.openai.embeddings.create({
      model: "text-embedding-ada-002",
      input: text,
    });
    return response.data[0].embedding;
  }

  /**
   * ä»£ç ç‰‡æ®µå‘é‡åŒ–å­˜å‚¨
   */
  async storeCodeEmbedding(
    projectId: string,
    filePath: string,
    codeSnippet: string,
    contentType: string,
    metadata: any = {}
  ) {
    // ç”Ÿæˆæè¿°
    const description = await this.generateCodeDescription(codeSnippet);

    // ç”Ÿæˆå‘é‡
    const embedding = await this.generateEmbedding(
      `${description}\n\n${codeSnippet}`
    );

    return await this.supabase.from("code_embeddings").insert({
      project_id: projectId,
      file_path: filePath,
      content_type: contentType,
      code_snippet: codeSnippet,
      description,
      embedding,
      metadata,
      tags: this.extractTags(codeSnippet, contentType),
    });
  }

  /**
   * é¡¹ç›®ä¸Šä¸‹æ–‡å‘é‡åŒ–å­˜å‚¨
   */
  async storeProjectContext(
    projectId: string,
    contextType: string,
    content: string,
    importance: number = 0.5
  ) {
    const summary = await this.generateContextSummary(content, contextType);
    const embedding = await this.generateEmbedding(
      `${contextType}: ${summary}`
    );

    return await this.supabase.from("project_context_embeddings").insert({
      project_id: projectId,
      context_type: contextType,
      content,
      summary,
      embedding,
      importance_score: importance,
    });
  }

  /**
   * æ™ºèƒ½ä»£ç æ£€ç´¢
   */
  async searchRelevantCode(
    projectId: string,
    query: string,
    limit: number = 5,
    threshold: number = 0.7
  ) {
    const queryEmbedding = await this.generateEmbedding(query);

    const { data, error } = await this.supabase.rpc("search_code_embeddings", {
      project_id: projectId,
      query_embedding: queryEmbedding,
      match_threshold: threshold,
      match_count: limit,
    });

    return data || [];
  }

  /**
   * é¡¹ç›®ä¸Šä¸‹æ–‡æ£€ç´¢
   */
  async getRelevantContext(
    projectId: string,
    userIntent: string,
    maxTokens: number = 2000
  ) {
    const queryEmbedding = await this.generateEmbedding(userIntent);

    const { data } = await this.supabase.rpc("search_project_context", {
      project_id: projectId,
      query_embedding: queryEmbedding,
      max_tokens: maxTokens,
    });

    return this.optimizeContextForTokens(data, maxTokens);
  }

  private async generateCodeDescription(code: string): Promise<string> {
    // ä½¿ç”¨è½»é‡çº§ LLM ç”Ÿæˆä»£ç æè¿°
    try {
      const response = await this.openai.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: [
          {
            role: "user",
            content: `ç®€æ´æè¿°è¿™æ®µä»£ç çš„åŠŸèƒ½ï¼ˆé™åˆ¶50å­—å†…ï¼‰ï¼š\n${code.substring(
              0,
              500
            )}`,
          },
        ],
        max_tokens: 100,
      });
      return response.choices[0].message.content || "ä»£ç ç‰‡æ®µ";
    } catch {
      return "ä»£ç ç‰‡æ®µ";
    }
  }
}
```

### 3. é¡¹ç›®å‘é‡åŒ–ç®¡ç†å™¨

```typescript
// src/lib/vector/project-vectorizer.ts
export class ProjectVectorizer {
  private embeddingService: EmbeddingService;

  constructor() {
    this.embeddingService = new EmbeddingService();
  }

  /**
   * å‘é‡åŒ–æ•´ä¸ªé¡¹ç›®
   */
  async vectorizeProject(projectId: string, projectPath: string) {
    console.log(`ğŸ” å¼€å§‹å‘é‡åŒ–é¡¹ç›®: ${projectId}`);

    // 1. æ‰«æå¹¶å‘é‡åŒ–ä»£ç æ–‡ä»¶
    await this.vectorizeCodeFiles(projectId, projectPath);

    // 2. å‘é‡åŒ–é¡¹ç›®é…ç½®
    await this.vectorizeProjectConfig(projectId, projectPath);

    // 3. å‘é‡åŒ–ç»„ä»¶åº“ä¿¡æ¯
    await this.vectorizeComponentLibrary(projectId, projectPath);

    // 4. å‘é‡åŒ–APIè·¯ç”±
    await this.vectorizeAPIRoutes(projectId, projectPath);

    console.log(`âœ… é¡¹ç›®å‘é‡åŒ–å®Œæˆ: ${projectId}`);
  }

  /**
   * å‘é‡åŒ–ä»£ç æ–‡ä»¶
   */
  private async vectorizeCodeFiles(projectId: string, projectPath: string) {
    const files = await this.scanCodeFiles(projectPath);

    for (const file of files) {
      const content = await fs.readFile(file.path, "utf-8");

      // æŒ‰å‡½æ•°/ç»„ä»¶åˆ†å‰²ä»£ç 
      const codeBlocks = this.parseCodeBlocks(content, file.extension);

      for (const block of codeBlocks) {
        await this.embeddingService.storeCodeEmbedding(
          projectId,
          file.relativePath,
          block.code,
          block.type,
          {
            language: file.extension,
            lineStart: block.lineStart,
            lineEnd: block.lineEnd,
            exports: block.exports,
          }
        );
      }
    }
  }

  /**
   * è§£æä»£ç å—
   */
  private parseCodeBlocks(content: string, extension: string): CodeBlock[] {
    const blocks: CodeBlock[] = [];

    if (extension === "tsx" || extension === "jsx") {
      // è§£æ React ç»„ä»¶
      blocks.push(...this.parseReactComponents(content));
      blocks.push(...this.parseReactHooks(content));
    }

    if (extension === "ts" || extension === "js") {
      // è§£æå‡½æ•°å’Œç±»
      blocks.push(...this.parseFunctions(content));
      blocks.push(...this.parseClasses(content));
    }

    return blocks;
  }

  /**
   * å¢é‡æ›´æ–°å‘é‡
   */
  async updateFileVectors(
    projectId: string,
    filePath: string,
    content: string
  ) {
    // åˆ é™¤æ—§å‘é‡
    await this.embeddingService.deleteFileVectors(projectId, filePath);

    // ç”Ÿæˆæ–°å‘é‡
    const blocks = this.parseCodeBlocks(content, path.extname(filePath));

    for (const block of blocks) {
      await this.embeddingService.storeCodeEmbedding(
        projectId,
        filePath,
        block.code,
        block.type
      );
    }
  }
}
```

### 4. æ™ºèƒ½ä¸Šä¸‹æ–‡æ£€ç´¢å™¨

```typescript
// src/lib/vector/context-retriever.ts
export class ContextRetriever {
  private embeddingService: EmbeddingService;
  private tokenCounter: TokenCounter;

  constructor() {
    this.embeddingService = new EmbeddingService();
    this.tokenCounter = new TokenCounter();
  }

  /**
   * ä¸º LLM æ„å»ºä¼˜åŒ–çš„ä¸Šä¸‹æ–‡
   */
  async buildOptimizedContext(
    projectId: string,
    userRequest: string,
    maxTokens: number = 4000
  ): Promise<OptimizedContext> {
    // 1. åˆ†æç”¨æˆ·æ„å›¾
    const intent = await this.analyzeUserIntent(userRequest);

    // 2. æ£€ç´¢ç›¸å…³ä»£ç ç‰‡æ®µ
    const relevantCode = await this.embeddingService.searchRelevantCode(
      projectId,
      userRequest,
      10,
      0.7
    );

    // 3. æ£€ç´¢é¡¹ç›®ä¸Šä¸‹æ–‡
    const projectContext = await this.embeddingService.getRelevantContext(
      projectId,
      userRequest,
      maxTokens * 0.3 // 30% ç»™é¡¹ç›®ä¸Šä¸‹æ–‡
    );

    // 4. æ£€ç´¢ç›¸å…³ç»„ä»¶
    const relevantComponents = await this.getRelevantComponents(
      intent.components,
      maxTokens * 0.2 // 20% ç»™ç»„ä»¶ä¿¡æ¯
    );

    // 5. æ£€ç´¢å†å²å¯¹è¯
    const conversationHistory = await this.getRelevantHistory(
      intent.conversationId,
      userRequest,
      maxTokens * 0.1 // 10% ç»™å†å²å¯¹è¯
    );

    // 6. æ„å»ºä¼˜åŒ–çš„ä¸Šä¸‹æ–‡
    return this.assembleContext({
      userRequest,
      relevantCode,
      projectContext,
      relevantComponents,
      conversationHistory,
      maxTokens,
    });
  }

  /**
   * ç»„è£…ä¼˜åŒ–çš„ä¸Šä¸‹æ–‡
   */
  private assembleContext(params: ContextParams): OptimizedContext {
    const context = {
      summary: this.generateProjectSummary(params.projectContext),
      relevantCode: params.relevantCode.slice(0, 5), // æœ€å¤š5ä¸ªä»£ç ç‰‡æ®µ
      componentGuide: this.generateComponentGuide(params.relevantComponents),
      previousContext: this.summarizeHistory(params.conversationHistory),
      suggestions: this.generateSuggestions(params.userRequest),
    };

    // ç¡®ä¿ä¸è¶…è¿‡ token é™åˆ¶
    return this.optimizeForTokenLimit(context, params.maxTokens);
  }

  /**
   * Token ä¼˜åŒ–
   */
  private optimizeForTokenLimit(
    context: any,
    maxTokens: number
  ): OptimizedContext {
    let totalTokens = this.tokenCounter.count(JSON.stringify(context));

    while (totalTokens > maxTokens) {
      // é€æ­¥å‡å°‘å†…å®¹
      if (context.relevantCode.length > 3) {
        context.relevantCode.pop();
      } else if (context.previousContext.length > 200) {
        context.previousContext = context.previousContext.substring(0, 200);
      } else if (context.componentGuide.length > 500) {
        context.componentGuide = context.componentGuide.substring(0, 500);
      } else {
        break;
      }

      totalTokens = this.tokenCounter.count(JSON.stringify(context));
    }

    return {
      ...context,
      tokenCount: totalTokens,
      optimizationApplied:
        totalTokens < this.tokenCounter.count(JSON.stringify(context)),
    };
  }
}
```

### 5. API é›†æˆ

```typescript
// src/app/api/vector/sync/route.ts
export async function POST(request: Request) {
  try {
    const { projectId, files, action } = await request.json();

    const vectorizer = new ProjectVectorizer();

    switch (action) {
      case "full_sync":
        await vectorizer.vectorizeProject(projectId, "sandbox");
        break;

      case "incremental_sync":
        for (const [filePath, content] of Object.entries(files)) {
          await vectorizer.updateFileVectors(projectId, filePath, content);
        }
        break;
    }

    return NextResponse.json({
      success: true,
      message: "å‘é‡åŒæ­¥å®Œæˆ",
    });
  } catch (error) {
    return NextResponse.json(
      {
        success: false,
        error: error.message,
      },
      { status: 500 }
    );
  }
}

// src/app/api/ai/generate-optimized/route.ts
export async function POST(request: Request) {
  try {
    const { prompt, projectId, conversationId } = await request.json();

    const contextRetriever = new ContextRetriever();

    // æ„å»ºä¼˜åŒ–çš„ä¸Šä¸‹æ–‡
    const optimizedContext = await contextRetriever.buildOptimizedContext(
      projectId,
      prompt,
      4000 // 4k tokens é™åˆ¶
    );

    // ä½¿ç”¨ä¼˜åŒ–çš„ä¸Šä¸‹æ–‡è°ƒç”¨ Dify
    const difyClient = DifyClient.getInstance();
    const result = await difyClient.generateUI(prompt, {
      projectType: "nextjs",
      context: JSON.stringify(optimizedContext),
      conversationId,
    });

    // å­˜å‚¨å¯¹è¯å†å²å‘é‡
    await new EmbeddingService().storeConversationHistory(
      conversationId,
      projectId,
      prompt,
      result.description
    );

    return NextResponse.json({
      success: true,
      data: result,
      contextOptimization: {
        tokensSaved: 8000 - optimizedContext.tokenCount,
        optimizationApplied: optimizedContext.optimizationApplied,
      },
    });
  } catch (error) {
    return NextResponse.json(
      {
        success: false,
        error: error.message,
      },
      { status: 500 }
    );
  }
}
```

## ğŸš€ å®æ–½æ­¥éª¤

### 1. ç¯å¢ƒé…ç½®

```bash
# å®‰è£…ä¾èµ–
npm install @supabase/supabase-js openai tiktoken

# ç¯å¢ƒå˜é‡
SUPABASE_URL=your_supabase_url
SUPABASE_ANON_KEY=your_supabase_anon_key
SUPABASE_SERVICE_KEY=your_supabase_service_key
OPENAI_API_KEY=your_openai_api_key
```

### 2. Supabase è®¾ç½®

1. åœ¨ Supabase é¡¹ç›®ä¸­å¯ç”¨å‘é‡æ‰©å±•
2. è¿è¡Œ SQL è„šæœ¬åˆ›å»ºè¡¨ç»“æ„
3. é…ç½® RLS (Row Level Security) ç­–ç•¥
4. åˆ›å»ºå‘é‡æœç´¢å‡½æ•°

### 3. é¡¹ç›®é›†æˆ

1. é›†æˆå‘é‡åŒ–æœåŠ¡åˆ°é¡¹ç›®ç®¡ç†å™¨
2. ä¿®æ”¹ AI ç”Ÿæˆ API ä½¿ç”¨ä¼˜åŒ–ä¸Šä¸‹æ–‡
3. æ·»åŠ å®æ—¶å‘é‡åŒæ­¥æœºåˆ¶
4. å®ç°å‘é‡æœç´¢ UI ç»„ä»¶

## ğŸ“Š é¢„æœŸæ”¶ç›Š

- **Token ä½¿ç”¨ä¼˜åŒ–**: å‡å°‘ 60-80% çš„ token æ¶ˆè€—
- **å“åº”é€Ÿåº¦æå‡**: æ›´ç²¾å‡†çš„ä¸Šä¸‹æ–‡æ£€ç´¢
- **çŸ¥è¯†ç§¯ç´¯**: é¡¹ç›®çŸ¥è¯†åº“æŒç»­å­¦ä¹ 
- **æˆæœ¬é™ä½**: æ˜¾è‘—å‡å°‘ LLM API è°ƒç”¨æˆæœ¬

## ğŸ” ç›‘æ§å’Œè°ƒä¼˜

1. **å‘é‡è´¨é‡ç›‘æ§**: æ£€ç´¢å‡†ç¡®ç‡è·Ÿè¸ª
2. **Token ä½¿ç”¨åˆ†æ**: ä¼˜åŒ–æ•ˆæœé‡åŒ–
3. **å“åº”æ—¶é—´ä¼˜åŒ–**: å‘é‡æ£€ç´¢æ€§èƒ½è°ƒä¼˜
4. **ç”¨æˆ·ä½“éªŒ**: A/B æµ‹è¯•å¯¹æ¯”æ•ˆæœ
