import { NextRequest, NextResponse } from 'next/server';
import { DifyClient } from '@/lib/ai/dify-client';
import { ContextRetriever } from '@/lib/vector/context-retriever';
import { EmbeddingService } from '@/lib/vector/embedding-service';
import { ProjectManager } from '@/lib/project-manager';

export async function POST(request: NextRequest) {
  try {
    const { 
      prompt, 
      projectId = 'sandbox-project', 
      conversationId,
      maxTokens = 4000,
      useOptimizedContext = true
    } = await request.json();

    if (!prompt) {
      return NextResponse.json(
        { success: false, error: 'è¯·æä¾›ä»£ç ç”Ÿæˆæç¤º' },
        { status: 400 }
      );
    }

    console.log(`ğŸ¤– å¼€å§‹ä¼˜åŒ–çš„ AI ä»£ç ç”Ÿæˆ: ${prompt.substring(0, 50)}...`);
    console.log(`ğŸ“Š ä½¿ç”¨ä¼˜åŒ–ä¸Šä¸‹æ–‡: ${useOptimizedContext}, æœ€å¤§tokens: ${maxTokens}`);

    // 1. åˆå§‹åŒ–æœåŠ¡
    const contextRetriever = new ContextRetriever();
    const embeddingService = new EmbeddingService();
    const projectManager = ProjectManager.getInstance();

    let optimizedContext;
    let contextOptimization = {
      tokensSaved: 0,
      optimizationApplied: false,
      originalTokenCount: 0,
      finalTokenCount: 0
    };

    if (useOptimizedContext) {
      // 2. æ„å»ºä¼˜åŒ–çš„ä¸Šä¸‹æ–‡
      console.log('ğŸ§  æ„å»ºä¼˜åŒ–ä¸Šä¸‹æ–‡...');
      optimizedContext = await contextRetriever.buildOptimizedContext(
        projectId,
        prompt,
        maxTokens,
        conversationId
      );

      contextOptimization = {
        tokensSaved: (optimizedContext.debugInfo?.originalTokenCount || 0) - optimizedContext.tokenCount,
        optimizationApplied: optimizedContext.optimizationApplied,
        originalTokenCount: optimizedContext.debugInfo?.originalTokenCount || 0,
        finalTokenCount: optimizedContext.tokenCount
      };

      console.log(`ğŸ“ˆ ä¸Šä¸‹æ–‡ä¼˜åŒ–ç»“æœ: åŸå§‹ ${contextOptimization.originalTokenCount} tokens â†’ æœ€ç»ˆ ${contextOptimization.finalTokenCount} tokens (èŠ‚çœ ${contextOptimization.tokensSaved} tokens)`);
    } else {
      console.log('âš ï¸ ä½¿ç”¨ä¼ ç»Ÿä¸Šä¸‹æ–‡ç”Ÿæˆæ¨¡å¼');
    }

    // 3. è°ƒç”¨ Dify ç”Ÿæˆä»£ç 
    console.log('ğŸš€ è°ƒç”¨ Dify API...');
    const difyClient = getDifyClient();
    
    const generateOptions = {
      projectType: 'nextjs' as const,
      context: useOptimizedContext ? JSON.stringify(optimizedContext) : `é¡¹ç›®ID: ${projectId}, ç›®æ ‡æ¡†æ¶: nextjs`,
      conversationId: conversationId || `opt-${Date.now()}`
    };

    const result = await difyClient.generateUI(prompt, generateOptions);

    console.log(`âœ… Dify ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ ${result.files.length} ä¸ªæ–‡ä»¶`);

    // 4. å°†ç”Ÿæˆçš„ä»£ç å†™å…¥ sandbox
    console.log('ğŸ’¾ å†™å…¥æ–‡ä»¶åˆ° sandbox...');
    const fileOperations = result.files.map(async (file) => {
      console.log(`ğŸ“ å†™å…¥æ–‡ä»¶: ${file.path}`);
      return projectManager.saveProjectFiles(projectId, {
        [file.path]: file.content
      });
    });

    await Promise.all(fileOperations);

    // 5. æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å¯é¡¹ç›®
    const hasConfigChanges = result.files.some(file =>
      file.path.includes('package.json') ||
      file.path.includes('next.config') ||
      file.path.includes('tailwind.config')
    );

    if (hasConfigChanges) {
      console.log('ğŸ“¦ æ£€æµ‹åˆ°é…ç½®æ–‡ä»¶å˜åŒ–ï¼Œå¯èƒ½éœ€è¦é‡å¯é¡¹ç›®');
    }

    // 6. å­˜å‚¨å¯¹è¯å†å²å‘é‡ï¼ˆå¼‚æ­¥ï¼Œä¸é˜»å¡å“åº”ï¼‰
    if (conversationId && useOptimizedContext) {
      embeddingService.storeConversationHistory({
        conversation_id: conversationId,
        project_id: projectId,
        user_intent: prompt,
        ai_response_summary: result.description || 'ç”Ÿæˆäº†UIç»„ä»¶',
        tokens_saved: contextOptimization.tokensSaved
      }).catch(error => {
        console.warn('å­˜å‚¨å¯¹è¯å†å²å¤±è´¥:', error);
      });
    }

    // 7. å¼‚æ­¥æ›´æ–°é¡¹ç›®å‘é‡ï¼ˆä¸é˜»å¡å“åº”ï¼‰
    if (useOptimizedContext) {
      updateProjectVectors(projectId, result.files).catch(error => {
        console.warn('æ›´æ–°é¡¹ç›®å‘é‡å¤±è´¥:', error);
      });
    }

    console.log('ğŸ‰ ä¼˜åŒ–çš„ä»£ç ç”Ÿæˆå®Œæˆï¼');

    return NextResponse.json({
      success: true,
      message: 'ğŸ‰ AI ä»£ç ç”Ÿæˆå¹¶å†™å…¥å®Œæˆï¼',
      data: {
        filesGenerated: result.files.length,
        files: result.files.map(f => ({
          path: f.path,
          size: f.content.length,
          type: f.type
        })),
        description: result.description,
        features: result.features,
        dependencies: result.dependencies,
        hasConfigChanges,
        conversationId: generateOptions.conversationId,
        // ä¼˜åŒ–ç›¸å…³ä¿¡æ¯
        optimization: {
          enabled: useOptimizedContext,
          ...contextOptimization,
          debugInfo: optimizedContext?.debugInfo
        }
      }
    });

  } catch (error) {
    console.error('ä¼˜åŒ–çš„ AI ä»£ç ç”Ÿæˆå¤±è´¥:', error);
    return NextResponse.json({
      success: false,
      error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯',
      details: error instanceof Error ? error.stack : undefined
    }, { status: 500 });
  }
}

/**
 * è·å–é…ç½®çŠ¶æ€
 */
export async function GET(request: NextRequest) {
  try {
    const { searchParams } = new URL(request.url);
    const action = searchParams.get('action') || 'status';

    switch (action) {
      case 'status':
        // æ£€æŸ¥å„é¡¹æœåŠ¡é…ç½®çŠ¶æ€
        const configStatus = {
          dify: {
            configured: !!process.env.DIFY_API_ENDPOINT && !!process.env.COMPONENT_DIFY_API_KEY,
            endpoint: process.env.DIFY_API_ENDPOINT ? 'å·²é…ç½®' : 'æœªé…ç½®'
          },
          openai: {
            configured: !!process.env.OPENAI_API_KEY,
            model: 'text-embedding-ada-002'
          },
          supabase: {
            configured: !!process.env.SUPABASE_URL && !!process.env.SUPABASE_SERVICE_KEY,
            url: process.env.SUPABASE_URL ? 'å·²é…ç½®' : 'æœªé…ç½®'
          },
          vectorization: {
            enabled: true,
            supportedTypes: ['component', 'function', 'type', 'style', 'config', 'api']
          }
        };

        const allConfigured = configStatus.dify.configured && 
                            configStatus.openai.configured && 
                            configStatus.supabase.configured;

        return NextResponse.json({
          success: true,
          data: {
            status: allConfigured ? 'ready' : 'needs_configuration',
            services: configStatus,
            recommendations: allConfigured ? [] : [
              !configStatus.dify.configured && 'è¯·é…ç½® DIFY_API_ENDPOINT å’Œ COMPONENT_DIFY_API_KEY',
              !configStatus.openai.configured && 'è¯·é…ç½® OPENAI_API_KEY',
              !configStatus.supabase.configured && 'è¯·é…ç½® SUPABASE_URL å’Œ SUPABASE_SERVICE_KEY'
            ].filter(Boolean)
          }
        });

      case 'test':
        // æµ‹è¯•å„é¡¹æœåŠ¡è¿æ¥
        const testResults = await testServices();
        return NextResponse.json({
          success: true,
          data: testResults
        });

      default:
        return NextResponse.json(
          { success: false, error: `ä¸æ”¯æŒçš„æ“ä½œ: ${action}` },
          { status: 400 }
        );
    }

  } catch (error) {
    console.error('è·å–ä¼˜åŒ–çŠ¶æ€å¤±è´¥:', error);
    return NextResponse.json({
      success: false,
      error: error instanceof Error ? error.message : 'è·å–çŠ¶æ€å¤±è´¥'
    }, { status: 500 });
  }
}

/**
 * åˆå§‹åŒ– Dify å®¢æˆ·ç«¯
 */
function getDifyClient(): DifyClient {
  const apiEndpoint = process.env.DIFY_API_ENDPOINT;

  if (!apiEndpoint) {
    throw new Error('è¯·è®¾ç½® DIFY_API_ENDPOINT ç¯å¢ƒå˜é‡');
  }

  return DifyClient.getInstance(apiEndpoint);
}

/**
 * å¼‚æ­¥æ›´æ–°é¡¹ç›®å‘é‡
 */
async function updateProjectVectors(projectId: string, files: any[]): Promise<void> {
  try {
    // æ„å»ºæ–‡ä»¶æ›´æ–°å¯¹è±¡
    const fileUpdates: { [path: string]: string } = {};
    files.forEach(file => {
      fileUpdates[file.path] = file.content;
    });

    // è°ƒç”¨å‘é‡åŒæ­¥API
    const response = await fetch(`${process.env.NEXT_PUBLIC_BASE_URL || 'http://localhost:3000'}/api/vector/sync`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        projectId,
        files: fileUpdates,
        action: 'incremental_sync'
      })
    });

    if (!response.ok) {
      throw new Error(`å‘é‡æ›´æ–°å¤±è´¥: ${response.statusText}`);
    }

    console.log('âœ… é¡¹ç›®å‘é‡æ›´æ–°å®Œæˆ');
  } catch (error) {
    console.error('å¼‚æ­¥å‘é‡æ›´æ–°å¤±è´¥:', error);
    throw error;
  }
}

/**
 * æµ‹è¯•å„é¡¹æœåŠ¡
 */
async function testServices(): Promise<any> {
  const results = {
    dify: { status: 'unknown', message: '', latency: 0 },
    openai: { status: 'unknown', message: '', latency: 0 },
    supabase: { status: 'unknown', message: '', latency: 0 }
  };

  // æµ‹è¯• OpenAI
  try {
    const start = Date.now();
    const embeddingService = new EmbeddingService();
    await embeddingService.generateEmbedding('æµ‹è¯•æ–‡æœ¬');
    results.openai = {
      status: 'ok',
      message: 'è¿æ¥æ­£å¸¸',
      latency: Date.now() - start
    };
  } catch (error) {
    results.openai = {
      status: 'error',
      message: error instanceof Error ? error.message : 'è¿æ¥å¤±è´¥',
      latency: 0
    };
  }

  // æµ‹è¯• Dify
  try {
    const start = Date.now();
    const difyClient = getDifyClient();
    // è¿™é‡Œå¯ä»¥æ·»åŠ  Dify è¿æ¥æµ‹è¯•
    results.dify = {
      status: 'ok',
      message: 'é…ç½®æ­£å¸¸',
      latency: Date.now() - start
    };
  } catch (error) {
    results.dify = {
      status: 'error',
      message: error instanceof Error ? error.message : 'é…ç½®é”™è¯¯',
      latency: 0
    };
  }

  // æµ‹è¯• Supabase
  try {
    const start = Date.now();
    const embeddingService = new EmbeddingService();
    // æµ‹è¯•æ•°æ®åº“è¿æ¥
    await embeddingService['supabase'].from('code_embeddings').select('id').limit(1);
    results.supabase = {
      status: 'ok',
      message: 'æ•°æ®åº“è¿æ¥æ­£å¸¸',
      latency: Date.now() - start
    };
  } catch (error) {
    results.supabase = {
      status: 'error',
      message: error instanceof Error ? error.message : 'æ•°æ®åº“è¿æ¥å¤±è´¥',
      latency: 0
    };
  }

  return results;
}
